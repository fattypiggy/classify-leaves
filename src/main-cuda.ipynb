{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:16:37.212610Z",
     "start_time": "2024-11-07T20:16:34.909581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# main.ipynb\n",
    "\n",
    "# import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision.models import resnet50, resnet101, ResNet50_Weights, ResNet101_Weights\n",
    "from torchvision.models.quantization import resnet50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:16:37.230513Z",
     "start_time": "2024-11-07T20:16:37.215513Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing and augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],   # ImageNet的均值和标准差\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],   # ImageNet的均值和标准差\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:16:37.297683Z",
     "start_time": "2024-11-07T20:16:37.279996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class LeafDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, mode='train'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.mode = mode  # 'train' or 'test'\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # extract labels and encode\n",
    "            self.le = LabelEncoder()\n",
    "            self.data['label'] = self.le.fit_transform(self.data['label'])\n",
    "            self.classes = self.le.classes_\n",
    "        else:\n",
    "            self.classes = None  # test set has no labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join('..', self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            label = self.data.iloc[idx, 1]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image, self.data.iloc[idx, 0]  # return image and file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:16:37.329336Z",
     "start_time": "2024-11-07T20:16:37.313757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class number: 176\n"
     ]
    }
   ],
   "source": [
    "# 加载训练数据集\n",
    "train_csv = '../train.csv'\n",
    "train_dataset = LeafDataset(csv_file=train_csv, transform=data_transforms['train'], mode='train')\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f'class number: {num_classes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:16:37.429383Z",
     "start_time": "2024-11-07T20:16:37.412550Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare K-fold cross validation\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.008\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "num_workers = 8\n",
    "checkpoint_dir = '../checkpoints'\n",
    "model_name = 'resnet50' # or resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:16:37.462651Z",
     "start_time": "2024-11-07T20:16:37.447583Z"
    }
   },
   "outputs": [],
   "source": [
    "# define training and validation function\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs, fold):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Fold {fold}, Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 每个epoch都有训练和验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # set model to training mode\n",
    "            else:\n",
    "                model.eval()   # set model to evaluation mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).long()\n",
    "\n",
    "                # zero parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward propagation\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backpropagation + optimization in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # learning rate adjustment\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            # calculate loss and accuracy\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Fold {fold} finished in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'best validation accuracy: {best_acc:.4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T22:35:57.593763Z",
     "start_time": "2024-11-07T20:16:37.480289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "--------------------\n",
      "Fold 0, Epoch 0/9\n",
      "----------\n",
      "Train Loss: 4.4075 Acc: 0.1214\n",
      "Val Loss: 2.4667 Acc: 0.3811\n",
      "\n",
      "Fold 0, Epoch 1/9\n",
      "----------\n",
      "Train Loss: 1.4517 Acc: 0.6204\n",
      "Val Loss: 0.9195 Acc: 0.7401\n",
      "\n",
      "Fold 0, Epoch 2/9\n",
      "----------\n",
      "Train Loss: 0.6377 Acc: 0.8201\n",
      "Val Loss: 0.6214 Acc: 0.8178\n",
      "\n",
      "Fold 0, Epoch 3/9\n",
      "----------\n",
      "Train Loss: 0.3890 Acc: 0.8905\n",
      "Val Loss: 0.4792 Acc: 0.8510\n",
      "\n",
      "Fold 0, Epoch 4/9\n",
      "----------\n",
      "Train Loss: 0.2761 Acc: 0.9187\n",
      "Val Loss: 0.3852 Acc: 0.8788\n",
      "\n",
      "Fold 0, Epoch 5/9\n",
      "----------\n",
      "Train Loss: 0.2001 Acc: 0.9429\n",
      "Val Loss: 0.2998 Acc: 0.9049\n",
      "\n",
      "Fold 0, Epoch 6/9\n",
      "----------\n",
      "Train Loss: 0.1566 Acc: 0.9556\n",
      "Val Loss: 0.2564 Acc: 0.9202\n",
      "\n",
      "Fold 0, Epoch 7/9\n",
      "----------\n",
      "Train Loss: 0.1034 Acc: 0.9751\n",
      "Val Loss: 0.2053 Acc: 0.9349\n",
      "\n",
      "Fold 0, Epoch 8/9\n",
      "----------\n",
      "Train Loss: 0.0846 Acc: 0.9786\n",
      "Val Loss: 0.1986 Acc: 0.9379\n",
      "\n",
      "Fold 0, Epoch 9/9\n",
      "----------\n",
      "Train Loss: 0.0827 Acc: 0.9792\n",
      "Val Loss: 0.1949 Acc: 0.9371\n",
      "\n",
      "Fold 0 finished in 13m 15s\n",
      "best validation accuracy: 0.9379\n",
      "Fold 1\n",
      "--------------------\n",
      "Fold 1, Epoch 0/9\n",
      "----------\n",
      "Train Loss: 4.4123 Acc: 0.1193\n",
      "Val Loss: 2.4627 Acc: 0.3754\n",
      "\n",
      "Fold 1, Epoch 1/9\n",
      "----------\n",
      "Train Loss: 1.4381 Acc: 0.6286\n",
      "Val Loss: 0.8977 Acc: 0.7333\n",
      "\n",
      "Fold 1, Epoch 2/9\n",
      "----------\n",
      "Train Loss: 0.6328 Acc: 0.8230\n",
      "Val Loss: 0.6285 Acc: 0.8069\n",
      "\n",
      "Fold 1, Epoch 3/9\n",
      "----------\n",
      "Train Loss: 0.3953 Acc: 0.8870\n",
      "Val Loss: 0.4854 Acc: 0.8507\n",
      "\n",
      "Fold 1, Epoch 4/9\n",
      "----------\n",
      "Train Loss: 0.2655 Acc: 0.9223\n",
      "Val Loss: 0.4200 Acc: 0.8690\n",
      "\n",
      "Fold 1, Epoch 5/9\n",
      "----------\n",
      "Train Loss: 0.2010 Acc: 0.9431\n",
      "Val Loss: 0.3903 Acc: 0.8837\n",
      "\n",
      "Fold 1, Epoch 6/9\n",
      "----------\n",
      "Train Loss: 0.1612 Acc: 0.9521\n",
      "Val Loss: 0.2652 Acc: 0.9109\n",
      "\n",
      "Fold 1, Epoch 7/9\n",
      "----------\n",
      "Train Loss: 0.1025 Acc: 0.9731\n",
      "Val Loss: 0.1889 Acc: 0.9368\n",
      "\n",
      "Fold 1, Epoch 8/9\n",
      "----------\n",
      "Train Loss: 0.0873 Acc: 0.9785\n",
      "Val Loss: 0.1958 Acc: 0.9333\n",
      "\n",
      "Fold 1, Epoch 9/9\n",
      "----------\n",
      "Train Loss: 0.0827 Acc: 0.9800\n",
      "Val Loss: 0.1989 Acc: 0.9352\n",
      "\n",
      "Fold 1 finished in 11m 44s\n",
      "best validation accuracy: 0.9368\n",
      "Fold 2\n",
      "--------------------\n",
      "Fold 2, Epoch 0/9\n",
      "----------\n",
      "Train Loss: 4.3903 Acc: 0.1296\n",
      "Val Loss: 2.4105 Acc: 0.3953\n",
      "\n",
      "Fold 2, Epoch 1/9\n",
      "----------\n",
      "Train Loss: 1.4268 Acc: 0.6340\n",
      "Val Loss: 0.9197 Acc: 0.7339\n",
      "\n",
      "Fold 2, Epoch 2/9\n",
      "----------\n",
      "Train Loss: 0.6336 Acc: 0.8252\n",
      "Val Loss: 0.5598 Acc: 0.8379\n",
      "\n",
      "Fold 2, Epoch 3/9\n",
      "----------\n",
      "Train Loss: 0.3841 Acc: 0.8890\n",
      "Val Loss: 0.5044 Acc: 0.8488\n",
      "\n",
      "Fold 2, Epoch 4/9\n",
      "----------\n",
      "Train Loss: 0.2742 Acc: 0.9223\n",
      "Val Loss: 0.4043 Acc: 0.8791\n",
      "\n",
      "Fold 2, Epoch 5/9\n",
      "----------\n",
      "Train Loss: 0.1973 Acc: 0.9425\n",
      "Val Loss: 0.3558 Acc: 0.8899\n",
      "\n",
      "Fold 2, Epoch 6/9\n",
      "----------\n",
      "Train Loss: 0.1519 Acc: 0.9558\n",
      "Val Loss: 0.3035 Acc: 0.9109\n",
      "\n",
      "Fold 2, Epoch 7/9\n",
      "----------\n",
      "Train Loss: 0.1013 Acc: 0.9750\n",
      "Val Loss: 0.2097 Acc: 0.9352\n",
      "\n",
      "Fold 2, Epoch 8/9\n",
      "----------\n",
      "Train Loss: 0.0849 Acc: 0.9798\n",
      "Val Loss: 0.2028 Acc: 0.9365\n",
      "\n",
      "Fold 2, Epoch 9/9\n",
      "----------\n",
      "Train Loss: 0.0772 Acc: 0.9813\n",
      "Val Loss: 0.1916 Acc: 0.9379\n",
      "\n",
      "Fold 2 finished in 10m 35s\n",
      "best validation accuracy: 0.9379\n",
      "Fold 3\n",
      "--------------------\n",
      "Fold 3, Epoch 0/9\n",
      "----------\n",
      "Train Loss: 4.4506 Acc: 0.1124\n",
      "Val Loss: 2.4708 Acc: 0.3632\n",
      "\n",
      "Fold 3, Epoch 1/9\n",
      "----------\n",
      "Train Loss: 1.4709 Acc: 0.6147\n",
      "Val Loss: 0.8733 Acc: 0.7561\n",
      "\n",
      "Fold 3, Epoch 2/9\n",
      "----------\n",
      "Train Loss: 0.6552 Acc: 0.8143\n",
      "Val Loss: 0.6061 Acc: 0.8185\n",
      "\n",
      "Fold 3, Epoch 3/9\n",
      "----------\n",
      "Train Loss: 0.4059 Acc: 0.8835\n",
      "Val Loss: 0.4329 Acc: 0.8665\n",
      "\n",
      "Fold 3, Epoch 4/9\n",
      "----------\n",
      "Train Loss: 0.2820 Acc: 0.9189\n",
      "Val Loss: 0.4318 Acc: 0.8610\n",
      "\n",
      "Fold 3, Epoch 5/9\n",
      "----------\n",
      "Train Loss: 0.2051 Acc: 0.9397\n",
      "Val Loss: 0.2849 Acc: 0.9025\n",
      "\n",
      "Fold 3, Epoch 6/9\n",
      "----------\n",
      "Train Loss: 0.1638 Acc: 0.9527\n",
      "Val Loss: 0.2739 Acc: 0.9074\n",
      "\n",
      "Fold 3, Epoch 7/9\n",
      "----------\n",
      "Train Loss: 0.1088 Acc: 0.9706\n",
      "Val Loss: 0.1870 Acc: 0.9409\n",
      "\n",
      "Fold 3, Epoch 8/9\n",
      "----------\n",
      "Train Loss: 0.0931 Acc: 0.9751\n",
      "Val Loss: 0.1890 Acc: 0.9376\n",
      "\n",
      "Fold 3, Epoch 9/9\n",
      "----------\n",
      "Train Loss: 0.0828 Acc: 0.9798\n",
      "Val Loss: 0.1774 Acc: 0.9414\n",
      "\n",
      "Fold 3 finished in 10m 17s\n",
      "best validation accuracy: 0.9414\n",
      "Fold 4\n",
      "--------------------\n",
      "Fold 4, Epoch 0/9\n",
      "----------\n",
      "Train Loss: 4.4733 Acc: 0.1119\n",
      "Val Loss: 2.6142 Acc: 0.3512\n",
      "\n",
      "Fold 4, Epoch 1/9\n",
      "----------\n",
      "Train Loss: 1.5378 Acc: 0.6051\n",
      "Val Loss: 1.0898 Acc: 0.6880\n",
      "\n",
      "Fold 4, Epoch 2/9\n",
      "----------\n",
      "Train Loss: 0.6569 Acc: 0.8161\n",
      "Val Loss: 0.6419 Acc: 0.8065\n",
      "\n",
      "Fold 4, Epoch 3/9\n",
      "----------\n",
      "Train Loss: 0.4025 Acc: 0.8842\n",
      "Val Loss: 0.4864 Acc: 0.8520\n",
      "\n",
      "Fold 4, Epoch 4/9\n",
      "----------\n",
      "Train Loss: 0.2728 Acc: 0.9230\n",
      "Val Loss: 0.3663 Acc: 0.8858\n",
      "\n",
      "Fold 4, Epoch 5/9\n",
      "----------\n",
      "Train Loss: 0.2114 Acc: 0.9374\n",
      "Val Loss: 0.3503 Acc: 0.8896\n",
      "\n",
      "Fold 4, Epoch 6/9\n",
      "----------\n",
      "Train Loss: 0.1636 Acc: 0.9513\n",
      "Val Loss: 0.3192 Acc: 0.8975\n",
      "\n",
      "Fold 4, Epoch 7/9\n",
      "----------\n",
      "Train Loss: 0.1020 Acc: 0.9737\n",
      "Val Loss: 0.2176 Acc: 0.9292\n",
      "\n",
      "Fold 4, Epoch 8/9\n",
      "----------\n",
      "Train Loss: 0.0886 Acc: 0.9775\n",
      "Val Loss: 0.2154 Acc: 0.9302\n",
      "\n",
      "Fold 4, Epoch 9/9\n",
      "----------\n",
      "Train Loss: 0.0814 Acc: 0.9800\n",
      "Val Loss: 0.2036 Acc: 0.9384\n",
      "\n",
      "Fold 4 finished in 10m 1s\n",
      "best validation accuracy: 0.9384\n"
     ]
    }
   ],
   "source": [
    "# start K-fold cross validation\n",
    "fold_results = {}\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
    "    print(f'Fold {fold}')\n",
    "    print('-' * 20)\n",
    "\n",
    "    # create data subsets\n",
    "    train_subsampler = Subset(train_dataset, train_idx)\n",
    "    val_subsampler = Subset(train_dataset, val_idx)\n",
    "\n",
    "    # data loaders\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(train_subsampler, batch_size=batch_size, shuffle=True, pin_memory=True),\n",
    "        'val': DataLoader(val_subsampler, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    }\n",
    "\n",
    "    # initialize model\n",
    "    # model_list = [\n",
    "    #     {'name': 'resnet50', 'model_fn': resnet50, 'weights': ResNet50_Weights.DEFAULT},\n",
    "    #     {'name': 'resnet101', 'model_fn': resnet101, 'weights': ResNet101_Weights.DEFAULT},\n",
    "    # ]\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    elif model_name == 'resnet101':\n",
    "        model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model: {model_name}')\n",
    "\n",
    "    # modify the last fully connected layer\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # set optimizer and learning rate scheduler\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # reduce learning rate by 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    # train and evaluate\n",
    "    model, best_acc = train_model(model, dataloaders, criterion, optimizer, exp_lr_scheduler, num_epochs, fold)\n",
    "\n",
    "    # save the best model for each fold\n",
    "    model_dir = os.path.join(checkpoint_dir, model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_save_path = os.path.join(model_dir, f'model_fold_{fold}.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # store results\n",
    "    fold_results[fold] = {'model': model, 'best_acc': best_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T22:35:57.624969Z",
     "start_time": "2024-11-07T22:35:57.610956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all folds training completed.\n"
     ]
    }
   ],
   "source": [
    "# all folds training completed\n",
    "print('all folds training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T22:35:57.716957Z",
     "start_time": "2024-11-07T22:35:57.713859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model from Fold 3, validation accuracy: 0.9414\n"
     ]
    }
   ],
   "source": [
    "# select the best model\n",
    "best_fold = max(fold_results, key=lambda x: fold_results[x]['best_acc'])\n",
    "best_model = fold_results[best_fold]['model']\n",
    "print(f'best model from Fold {best_fold}, validation accuracy: {fold_results[best_fold][\"best_acc\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T22:36:13.939814Z",
     "start_time": "2024-11-07T22:35:57.794441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction results saved to sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# load test set\n",
    "test_csv = '../test.csv'\n",
    "test_dataset = LeafDataset(csv_file=test_csv, transform=data_transforms['test'], mode='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# use the best model for prediction\n",
    "best_model.eval()\n",
    "all_preds = []\n",
    "image_names = []\n",
    "with torch.no_grad():\n",
    "    for inputs, img_paths in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = best_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        image_names.extend(img_paths)\n",
    "\n",
    "# map predicted labels back to original labels\n",
    "label_encoder = train_dataset.le  # get LabelEncoder from training set\n",
    "predicted_labels = label_encoder.inverse_transform(all_preds)\n",
    "\n",
    "# create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'image': image_names,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# save as CSV file\n",
    "result_path = os.path.join(checkpoint_dir, model_name, 'sample_submission.csv')\n",
    "submission.to_csv(result_path, index=False)\n",
    "print('prediction results saved to sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T22:36:52.222572Z",
     "start_time": "2024-11-07T22:36:14.004144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction results saved to sample_submission-5fold.csv\n"
     ]
    }
   ],
   "source": [
    "# load test set\n",
    "test_csv = '../test.csv'  # please adjust the path according to实际情况\n",
    "test_dataset = LeafDataset(csv_file=test_csv, transform=data_transforms['test'], mode='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# use 5-fold models for prediction and voting\n",
    "from collections import Counter\n",
    "\n",
    "# ensure all models are in evaluation mode and loaded to GPU\n",
    "for fold in fold_results:\n",
    "    fold_results[fold]['model'].eval()\n",
    "    fold_results[fold]['model'] = fold_results[fold]['model'].to(device)\n",
    "\n",
    "# store prediction results of each model\n",
    "all_fold_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, img_paths in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        # store prediction results of each model in current batch\n",
    "        batch_fold_preds = []\n",
    "\n",
    "        # for each model, get prediction results\n",
    "        for fold in fold_results:\n",
    "            model = fold_results[fold]['model']\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            batch_fold_preds.append(preds.cpu().numpy())\n",
    "\n",
    "        # transpose all model prediction results, so each element corresponds to all model predictions for a sample\n",
    "        batch_fold_preds = np.array(batch_fold_preds)  # Shape: (num_folds, batch_size)\n",
    "        batch_fold_preds = batch_fold_preds.T  # Shape: (batch_size, num_folds)\n",
    "\n",
    "        # for each sample, perform voting\n",
    "        final_preds = []\n",
    "        for preds_per_sample in batch_fold_preds:\n",
    "            # count occurrences of each class\n",
    "            vote_counts = Counter(preds_per_sample)\n",
    "            # find the class with the most occurrences\n",
    "            most_common = vote_counts.most_common(1)[0][0]\n",
    "            final_preds.append(most_common)\n",
    "\n",
    "        # store final prediction results for current batch\n",
    "        all_fold_preds.extend(final_preds)\n",
    "\n",
    "# map predicted labels back to original labels\n",
    "label_encoder = train_dataset.le  # get LabelEncoder from training set\n",
    "predicted_labels = label_encoder.inverse_transform(all_fold_preds)\n",
    "\n",
    "# get image names from test set\n",
    "image_names = test_dataset.data['image'].tolist()\n",
    "\n",
    "# create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'image': image_names,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# save as CSV file\n",
    "result_path = os.path.join(checkpoint_dir, model_name, 'sample_submission-5fold.csv')\n",
    "submission.to_csv(result_path, index=False)\n",
    "print('prediction results saved to sample_submission-5fold.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
